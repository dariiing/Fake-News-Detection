{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import the preprocessed ISOT dataset",
   "id": "8c967188b7db9412"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:27.354100Z",
     "start_time": "2025-01-05T14:35:26.741620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={'train': \"preprocessed_isot/isot_train.csv\", 'test': \"preprocessed_isot/isot_test.csv\"})\n",
    "print(dataset)"
   ],
   "id": "35f3f8cb0d176635",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['statement', 'label'],\n",
      "        num_rows: 8578\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['statement', 'label'],\n",
      "        num_rows: 16713\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pick the pre-trained model",
   "id": "b517a9d59eb42e76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:27.358687Z",
     "start_time": "2025-01-05T14:35:27.355553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"albert/albert-base-v2\"\n",
    "your_path = 'isot_new_results'"
   ],
   "id": "7e5dd0e4d414bb8c",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Look over the label distribution",
   "id": "b4a1917732497053"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:27.375218Z",
     "start_time": "2025-01-05T14:35:27.359714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_label_distribution = Counter(dataset['train']['label'])\n",
    "test_label_distribution = Counter(dataset['test']['label'])\n",
    "\n",
    "print(\"Training Label Distribution:\", train_label_distribution)\n",
    "print(\"Test Label Distribution:\", test_label_distribution)"
   ],
   "id": "93a4c785ef92b56d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Distribution: Counter({False: 4310, True: 4268})\n",
      "Test Label Distribution: Counter({True: 8528, False: 8185})\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Labels in their original form are strings (true/false). We need to convert them to numerical values so they can be processed by the model.",
   "id": "22adf2174484ab0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:27.512591Z",
     "start_time": "2025-01-05T14:35:27.376224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(dataset['train']['label'])\n",
    "\n",
    "def encode_labels(example):\n",
    "    return {'encoded_label': label_encoder.transform([example['label']])[0]}\n",
    "\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].map(encode_labels, batched=False)"
   ],
   "id": "119cfede3e42a1a0",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The id2label and label2id mappings in AutoConfig are used to inform the model of the specific label-to-ID mappings so we can get the actual label names rather than the numerical reps when we do inference with the model.",
   "id": "5235ccf546e00f40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:27.734844Z",
     "start_time": "2025-01-05T14:35:27.513097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "unique_labels = sorted(list(set(dataset['train']['label'])))\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n"
   ],
   "id": "74e36377950e72f5",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Verify the correct labels are being used",
   "id": "c3ae7f0e40d930d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:27.738053Z",
     "start_time": "2025-01-05T14:35:27.734844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"ID to Label Mapping:\", config.id2label)\n",
    "print(\"Label to ID Mapping:\", config.label2id)"
   ],
   "id": "2408315af31a0c8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID to Label Mapping: {0: False, 1: True}\n",
      "Label to ID Mapping: {False: 0, True: 1}\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4302a4edd5f344dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we need to tokenize the text data so it can be processed by the model. We will use the tokenizer that corresponds to the model we are using from Hugging Face's Transformers library.",
   "id": "5474992b3860f0fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:28.052516Z",
     "start_time": "2025-01-05T14:35:27.739061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "model = AlbertForSequenceClassification.from_pretrained(model_name, config=config)\n"
   ],
   "id": "4e36576eb2017537",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to filter out any examples that have invalid content (e.g. empty strings) before tokenizing the data. Then, we will encode the labels and tokenize the text data. Tokenizing means converting the text data into numerical representations that can be processed by the model.",
   "id": "9c96fd96434c097a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:28.056825Z",
     "start_time": "2025-01-05T14:35:28.052516Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset)",
   "id": "4f276f3395cecd06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['statement', 'label', 'encoded_label'],\n",
      "        num_rows: 8578\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['statement', 'label', 'encoded_label'],\n",
      "        num_rows: 16713\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:28.179585Z",
     "start_time": "2025-01-05T14:35:28.057849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_invalid_content(example):\n",
    "    return isinstance(example['statement'], str)\n",
    "\n",
    "dataset = dataset.filter(filter_invalid_content, batched=False)\n",
    "\n",
    "def encode_data(batch):\n",
    "    tokenized_inputs = tokenizer(batch[\"statement\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    tokenized_inputs[\"labels\"] = batch[\"encoded_label\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "dataset_encoded = dataset.map(encode_data, batched=True)"
   ],
   "id": "7f27f8afbaeda6a5",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Verify the data is tokenized correctly:",
   "id": "973e8f2b24dbe374"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:28.185352Z",
     "start_time": "2025-01-05T14:35:28.181594Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset_encoded)",
   "id": "95fa4f366efcae17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['statement', 'label', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 8578\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['statement', 'label', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 16713\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:28.187930Z",
     "start_time": "2025-01-05T14:35:28.185352Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9d6236d3a5c8fe54",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:28.192371Z",
     "start_time": "2025-01-05T14:35:28.188398Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])",
   "id": "4f6306b0a5b21ba1",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The DataCollatorWithPadding ensures that all input sequences in a batch are padded to the same length, using the padding logic defined by the tokenizer. This is necessary because the model can only process inputs of the same length.",
   "id": "51966d61db6c9e28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:28.195572Z",
     "start_time": "2025-01-05T14:35:28.192371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ],
   "id": "44e5507bc487e0a0",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next we'll set up LabelEncoder to encode labels and defines a function to compute per-label accuracy from a confusion matrix, providing label-specific accuracy metrics. I.e. when we train the model we want to see the accuracy metrics per label as well as the average metrics. This is more relevant if you have more than two labels, and one is underperforming. ",
   "id": "b3b532b39e3f67bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:28.200582Z",
     "start_time": "2025-01-05T14:35:28.195572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "\n",
    "def per_label_accuracy(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    correct_predictions = cm.diagonal()\n",
    "    label_totals = cm.sum(axis=1)\n",
    "    per_label_acc = np.divide(correct_predictions, label_totals, out=np.zeros_like(correct_predictions, dtype=float), where=label_totals != 0)\n",
    "    return dict(zip(labels, per_label_acc))"
   ],
   "id": "6631dde6b29a5f95",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compute the following metrics: accuracy, precision, recall, f1 score, and per-label accuracy.",
   "id": "e1dbb72b88c986ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:28.205858Z",
     "start_time": "2025-01-05T14:35:28.200582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    decoded_labels = label_encoder.inverse_transform(labels)\n",
    "    decoded_preds = label_encoder.inverse_transform(preds)\n",
    "\n",
    "    precision = precision_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    recall = recall_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    acc = accuracy_score(decoded_labels, decoded_preds)\n",
    "\n",
    "    labels_list = list(label_encoder.classes_)\n",
    "    per_label_acc = per_label_accuracy(decoded_labels, decoded_preds, labels_list)\n",
    "\n",
    "    per_label_acc_metrics = {}\n",
    "    for label, accuracy in per_label_acc.items():\n",
    "        label_key = f\"accuracy_label_{label}\"\n",
    "        per_label_acc_metrics[label_key] = accuracy\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        **per_label_acc_metrics\n",
    "    }"
   ],
   "id": "dbc426b92f30f17e",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, the training begins. Training loss and validation loss should decrease consistenly. If the training loss is decreasing but the validation loss is increasing, the model is overfitting. If both are increasing, the model is underfitting.",
   "id": "89b117a85c9dd98e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:35:28.219318Z",
     "start_time": "2025-01-05T14:35:28.205858Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset_encoded['train'][0])\n",
   "id": "3e347236e4ba3167",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2, 13223,    15,  5766,     9,    13,     5,   139, 21458,    18,\n",
      "            6,    13,     8,   132,   148,    72,  2632,    59,    46, 20252,\n",
      "           35,    21,  5159,  7095,  6950,    19, 13223,    15,  4425,    15,\n",
      "          236,  1617,    57,  5263,    21,  8022,   149,    14,  2235,  3389,\n",
      "         2316,    15,  1148,    24,    13,     1,   108, 10424,    43,    21,\n",
      "        10597,     9,     1, 10064,  4396,    15,    13, 22940,  1046,    13,\n",
      "          103,  9967,  6401,    17,  1010,   334, 28357,  1346,  5263,    14,\n",
      "         8022,    27,  8775,    19,  6811,  3343,   495,    19, 13223,    15,\n",
      "         5865,    30,  7095,     1,    18,  4251,  3029,    16,    13,     1,\n",
      "         3060,   105,    70,     1,    35,    21,  6950,    35,    14,  4425,\n",
      "          294,  3087,   459,    27,   285,   137,    23,  2081,    20,  1744,\n",
      "           33,  5955,    20,    13,     1,  3699, 21095,    15, 20297,  1825,\n",
      "          558,    20,  4681, 14558,    15,   215,    14, 16328,    18,     9,\n",
      "            1,    14,  8022,    67,  1817,    28, 19270,    14,  7095,  1150,\n",
      "           17,   132,  1883,    72,    14, 16328,    18,  2632, 20252,   105,\n",
      "           35,    14,  6950,     9,    14,   132, 16328,    18,    15,    65,\n",
      "        13223,  2175,    15,  2632,    59,  2537,  1825,  5368,    15,  6090,\n",
      "        13402,    17, 23622,    15,    17,    50,  5246, 27005, 15184,    15,\n",
      "          496,    20,    14,  8022,     9,    13,     1, 18025,  1566,  4569,\n",
      "           25,    40,   189,  2876,    15,  1118,    19,    14,  4141,    16,\n",
      "         3389,  2634,    15,     1,  1685,  5742,    15,    14,  2925,    26,\n",
      "           14, 16328,    18,    15,   470, 27853,    19,    21, 10623,  2802,\n",
      "            9,    13,     1,   811,    98,    42,   196,    65,    84,    14,\n",
      "          475,    15,    85,    17,    85,   188,    15,    25,  3300,  3390,\n",
      "          149, 14558,    35,  7095,  1150,   963,     9,   158, 16328,    18,\n",
      "           50,  1148,    13,     1, 23680,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(1)}\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:41:21.765732Z",
     "start_time": "2025-01-05T14:35:28.219318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=1000,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_encoded['train'],\n",
    "    eval_dataset=dataset_encoded['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "af40704b3d471dce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 28/804 05:29 < 2:44:01, 0.08 it/s, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[140], line 28\u001B[0m\n\u001B[0;32m      3\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m      4\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./output\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      5\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m     gradient_accumulation_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m     16\u001B[0m )\n\u001B[0;32m     18\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     19\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     20\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m     data_collator\u001B[38;5;241m=\u001B[39mdata_collator,\n\u001B[0;32m     26\u001B[0m )\n\u001B[1;32m---> 28\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\fakes\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2164\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2162\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2165\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2169\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\fakes\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2524\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2517\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2518\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m   2519\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2520\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[0;32m   2521\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[0;32m   2522\u001B[0m )\n\u001B[0;32m   2523\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m-> 2524\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2527\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2528\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2529\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2530\u001B[0m ):\n\u001B[0;32m   2531\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2532\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\Desktop\\fakes\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3687\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   3685\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m   3686\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 3687\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3688\u001B[0m     \u001B[38;5;66;03m# Finally we need to normalize the loss for reporting\u001B[39;00m\n\u001B[0;32m   3689\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_items_in_batch \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Desktop\\fakes\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2248\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[1;34m(self, loss, **kwargs)\u001B[0m\n\u001B[0;32m   2246\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[0;32m   2247\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2248\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\fakes\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\fakes\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\fakes\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the results, the model and the state of the model.",
   "id": "312c99b9c0d8561d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer.evaluate()\n",
    "trainer.save_model(your_path)\n",
    "trainer.save_state()"
   ],
   "id": "2488383c06dfedb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "id": "3f23cd771109c911",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
