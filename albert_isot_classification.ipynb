{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c967188b7db9412",
   "metadata": {},
   "source": [
    "Import the preprocessed ISOT dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "35f3f8cb0d176635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:23.829041Z",
     "start_time": "2025-01-13T20:01:23.350270Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"./preprocessed_isot/isot_train.csv\",\n",
    "        \"test\": \"./preprocessed_isot/isot_test.csv\",\n",
    "        \"valid\": \"./preprocessed_isot/isot_valid.csv\",\n",
    "    },\n",
    ")\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['statement', 'label'],\n",
      "        num_rows: 31716\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['statement', 'label'],\n",
      "        num_rows: 4351\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['statement', 'label'],\n",
      "        num_rows: 4353\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "b517a9d59eb42e76",
   "metadata": {},
   "source": [
    "Pick the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "id": "7e5dd0e4d414bb8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:23.834048Z",
     "start_time": "2025-01-13T20:01:23.830049Z"
    }
   },
   "source": [
    "model_name = \"albert/albert-base-v2\"\n",
    "your_path = 'isot_new_results'"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "b4a1917732497053",
   "metadata": {},
   "source": [
    "Look over the label distribution"
   ]
  },
  {
   "cell_type": "code",
   "id": "93a4c785ef92b56d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:23.849213Z",
     "start_time": "2025-01-13T20:01:23.834552Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_label_distribution = Counter(dataset['train']['label'])\n",
    "test_label_distribution = Counter(dataset['test']['label'])\n",
    "\n",
    "print(\"Training Label Distribution:\", train_label_distribution)\n",
    "print(\"Test Label Distribution:\", test_label_distribution)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Distribution: Counter({True: 16992, False: 14724})\n",
      "Test Label Distribution: Counter({False: 2214, True: 2137})\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "22adf2174484ab0e",
   "metadata": {},
   "source": [
    "Labels in their original form are strings (true/false). We need to convert them to numerical values so they can be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "119cfede3e42a1a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:23.876324Z",
     "start_time": "2025-01-13T20:01:23.850220Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(dataset['train']['label'])\n",
    "\n",
    "def encode_labels(example):\n",
    "    return {'encoded_label': label_encoder.transform([example['label']])[0]}\n",
    "\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].map(encode_labels, batched=False)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "5235ccf546e00f40",
   "metadata": {},
   "source": [
    "The id2label and label2id mappings in AutoConfig are used to inform the model of the specific label-to-ID mappings so we can get the actual label names rather than the numerical reps when we do inference with the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "74e36377950e72f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.131441Z",
     "start_time": "2025-01-13T20:01:23.876324Z"
    }
   },
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "unique_labels = sorted(list(set(dataset['train']['label'])))\n",
    "id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "c3ae7f0e40d930d7",
   "metadata": {},
   "source": [
    "Verify the correct labels are being used"
   ]
  },
  {
   "cell_type": "code",
   "id": "2408315af31a0c8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.135456Z",
     "start_time": "2025-01-13T20:01:24.132452Z"
    }
   },
   "source": [
    "print(\"ID to Label Mapping:\", config.id2label)\n",
    "print(\"Label to ID Mapping:\", config.label2id)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID to Label Mapping: {0: False, 1: True}\n",
      "Label to ID Mapping: {False: 0, True: 1}\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "4302a4edd5f344dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5474992b3860f0fc",
   "metadata": {},
   "source": [
    "Now, we need to tokenize the text data so it can be processed by the model. We will use the tokenizer that corresponds to the model we are using from Hugging Face's Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "id": "4e36576eb2017537",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.608344Z",
     "start_time": "2025-01-13T20:01:24.136708Z"
    }
   },
   "source": [
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "model = AlbertForSequenceClassification.from_pretrained(model_name, config=config)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "2d7bc1ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.614275Z",
     "start_time": "2025-01-13T20:01:24.609894Z"
    }
   },
   "source": [
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlbertForSequenceClassification(\n",
      "  (albert): AlbertModel(\n",
      "    (embeddings): AlbertEmbeddings(\n",
      "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): AlbertTransformer(\n",
      "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
      "      (albert_layer_groups): ModuleList(\n",
      "        (0): AlbertLayerGroup(\n",
      "          (albert_layers): ModuleList(\n",
      "            (0): AlbertLayer(\n",
      "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): AlbertSdpaAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (attention_dropout): Dropout(p=0, inplace=False)\n",
      "                (output_dropout): Dropout(p=0, inplace=False)\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (pooler_activation): Tanh()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "259a598c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.619114Z",
     "start_time": "2025-01-13T20:01:24.615601Z"
    }
   },
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert.embeddings.word_embeddings.weight False\n",
      "albert.embeddings.position_embeddings.weight False\n",
      "albert.embeddings.token_type_embeddings.weight False\n",
      "albert.embeddings.LayerNorm.weight False\n",
      "albert.embeddings.LayerNorm.bias False\n",
      "albert.encoder.embedding_hidden_mapping_in.weight False\n",
      "albert.encoder.embedding_hidden_mapping_in.bias False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight False\n",
      "albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias False\n",
      "albert.pooler.weight False\n",
      "albert.pooler.bias False\n",
      "classifier.weight True\n",
      "classifier.bias True\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "9c96fd96434c097a",
   "metadata": {},
   "source": [
    "We need to filter out any examples that have invalid content (e.g. empty strings) before tokenizing the data. Then, we will encode the labels and tokenize the text data. Tokenizing means converting the text data into numerical representations that can be processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "4f276f3395cecd06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.623353Z",
     "start_time": "2025-01-13T20:01:24.620361Z"
    }
   },
   "source": [
    "print(dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['statement', 'label', 'encoded_label'],\n",
      "        num_rows: 31716\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['statement', 'label', 'encoded_label'],\n",
      "        num_rows: 4351\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['statement', 'label', 'encoded_label'],\n",
      "        num_rows: 4353\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "7f27f8afbaeda6a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.658063Z",
     "start_time": "2025-01-13T20:01:24.623353Z"
    }
   },
   "source": [
    "def filter_invalid_content(example):\n",
    "    return isinstance(example['statement'], str)\n",
    "\n",
    "dataset = dataset.filter(filter_invalid_content, batched=False)\n",
    "\n",
    "def encode_data(batch):\n",
    "    tokenized_inputs = tokenizer(batch[\"statement\"], truncation=True, max_length=256)\n",
    "    tokenized_inputs[\"labels\"] = batch[\"encoded_label\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "dataset_encoded = dataset.map(encode_data, batched=True)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "973e8f2b24dbe374",
   "metadata": {},
   "source": [
    "Verify the data is tokenized correctly:"
   ]
  },
  {
   "cell_type": "code",
   "id": "95fa4f366efcae17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.661945Z",
     "start_time": "2025-01-13T20:01:24.659025Z"
    }
   },
   "source": [
    "print(dataset_encoded)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['statement', 'label', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 31716\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['statement', 'label', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4351\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['statement', 'label', 'encoded_label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4353\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "4f6306b0a5b21ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.667593Z",
     "start_time": "2025-01-13T20:01:24.662975Z"
    }
   },
   "source": [
    "dataset_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "76c2984f",
   "metadata": {},
   "source": [
    "The DataCollatorWithPadding ensures that all input sequences in a batch are padded to the same length, using the padding logic defined by the tokenizer. This is necessary because the model can only process inputs of the same length."
   ]
  },
  {
   "cell_type": "code",
   "id": "e6c4e6b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.670678Z",
     "start_time": "2025-01-13T20:01:24.668634Z"
    }
   },
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "b3b532b39e3f67bb",
   "metadata": {},
   "source": [
    "Next we'll set up LabelEncoder to encode labels and defines a function to compute per-label accuracy from a confusion matrix, providing label-specific accuracy metrics. I.e. when we train the model we want to see the accuracy metrics per label as well as the average metrics. This is more relevant if you have more than two labels, and one is underperforming. "
   ]
  },
  {
   "cell_type": "code",
   "id": "6631dde6b29a5f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.675381Z",
     "start_time": "2025-01-13T20:01:24.671685Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "\n",
    "def per_label_accuracy(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    correct_predictions = cm.diagonal()\n",
    "    label_totals = cm.sum(axis=1)\n",
    "    per_label_acc = np.divide(correct_predictions, label_totals, out=np.zeros_like(correct_predictions, dtype=float), where=label_totals != 0)\n",
    "    return dict(zip(labels, per_label_acc))"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "e1dbb72b88c986ae",
   "metadata": {},
   "source": [
    "Compute the following metrics: accuracy, precision, recall, f1 score, and per-label accuracy."
   ]
  },
  {
   "cell_type": "code",
   "id": "dbc426b92f30f17e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.680308Z",
     "start_time": "2025-01-13T20:01:24.675381Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    decoded_labels = label_encoder.inverse_transform(labels)\n",
    "    decoded_preds = label_encoder.inverse_transform(preds)\n",
    "\n",
    "    precision = precision_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    recall = recall_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    f1 = f1_score(decoded_labels, decoded_preds, average='weighted')\n",
    "    acc = accuracy_score(decoded_labels, decoded_preds)\n",
    "\n",
    "    labels_list = list(label_encoder.classes_)\n",
    "    per_label_acc = per_label_accuracy(decoded_labels, decoded_preds, labels_list)\n",
    "\n",
    "    per_label_acc_metrics = {}\n",
    "    for label, accuracy in per_label_acc.items():\n",
    "        label_key = f\"accuracy_label_{label}\"\n",
    "        per_label_acc_metrics[label_key] = accuracy\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        **per_label_acc_metrics\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "89b117a85c9dd98e",
   "metadata": {},
   "source": [
    "Next, the training begins. Training loss and validation loss should decrease consistenly. If the training loss is decreasing but the validation loss is increasing, the model is overfitting. If both are increasing, the model is underfitting."
   ]
  },
  {
   "cell_type": "code",
   "id": "3e347236e4ba3167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:01:24.685671Z",
     "start_time": "2025-01-13T20:01:24.681337Z"
    }
   },
   "source": [
    "print(dataset_encoded['train'][0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2,  1529,   136,    13,     5,   139, 21458,    18,     6,    13,\n",
      "            8,    14,  2122,    26,  4397,    16,    40,  6559,    30,   841,\n",
      "         4541,    19,  1529,   136,  1272,    27,  8885,    28,  3655,  2004,\n",
      "         5863,    14,   358,    16,    14,   236,   840,   167,    20,    44,\n",
      "         2863,  2550,    14, 19541,    16,  7355,  1374,     9,  2454,  2797,\n",
      "          789, 11556,  1232,    58,    87,    65,    14,  2576,    41,    74,\n",
      "         5863,    37,    14,  8435,    16,    40,   488,   353,    19,    14,\n",
      "           71,  5093, 12254,   256,    16,    14,  1057,     9,    14,  2122,\n",
      "           35,    89,  1374,    30,  7355,  1272,   238,   509,  1464,     9,\n",
      "           19,   600,   203,  3680,   148,   440,    19,    14, 11131,     9,\n",
      "          732,  6559,    15,    14,   127,  9389,    19,    21,  2782,    15,\n",
      "           29,   557,    81,     8, 17124,    16,    14, 24064,    19,    14,\n",
      "         1057,     9,  1201,    30,  1617,    15,  8737,   148,   440,    76,\n",
      "           40,   166,    91,  2177, 22351,   770,    14,   180,    16,    14,\n",
      "          475,     9,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(1)}\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "af40704b3d471dce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:46:56.365843Z",
     "start_time": "2025-01-13T20:01:24.685671Z"
    }
   },
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_encoded['train'],\n",
    "    eval_dataset=dataset_encoded['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1485' max='1485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1485/1485 45:30, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy Label False</th>\n",
       "      <th>Accuracy Label True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.397400</td>\n",
       "      <td>0.579982</td>\n",
       "      <td>0.762583</td>\n",
       "      <td>0.752310</td>\n",
       "      <td>0.821954</td>\n",
       "      <td>0.762583</td>\n",
       "      <td>0.557362</td>\n",
       "      <td>0.975199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.104900</td>\n",
       "      <td>0.506388</td>\n",
       "      <td>0.866008</td>\n",
       "      <td>0.865020</td>\n",
       "      <td>0.880065</td>\n",
       "      <td>0.866008</td>\n",
       "      <td>0.775068</td>\n",
       "      <td>0.960225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.879900</td>\n",
       "      <td>0.451364</td>\n",
       "      <td>0.907607</td>\n",
       "      <td>0.907487</td>\n",
       "      <td>0.911223</td>\n",
       "      <td>0.907607</td>\n",
       "      <td>0.864047</td>\n",
       "      <td>0.952737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.681600</td>\n",
       "      <td>0.410379</td>\n",
       "      <td>0.924385</td>\n",
       "      <td>0.924347</td>\n",
       "      <td>0.926311</td>\n",
       "      <td>0.924385</td>\n",
       "      <td>0.893406</td>\n",
       "      <td>0.956481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.557300</td>\n",
       "      <td>0.373622</td>\n",
       "      <td>0.932429</td>\n",
       "      <td>0.932434</td>\n",
       "      <td>0.932486</td>\n",
       "      <td>0.932429</td>\n",
       "      <td>0.928636</td>\n",
       "      <td>0.936359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.425800</td>\n",
       "      <td>0.349255</td>\n",
       "      <td>0.941623</td>\n",
       "      <td>0.941623</td>\n",
       "      <td>0.942198</td>\n",
       "      <td>0.941623</td>\n",
       "      <td>0.925474</td>\n",
       "      <td>0.958353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.380300</td>\n",
       "      <td>0.326842</td>\n",
       "      <td>0.946679</td>\n",
       "      <td>0.946683</td>\n",
       "      <td>0.946817</td>\n",
       "      <td>0.946679</td>\n",
       "      <td>0.939476</td>\n",
       "      <td>0.954141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.278500</td>\n",
       "      <td>0.309469</td>\n",
       "      <td>0.949897</td>\n",
       "      <td>0.949900</td>\n",
       "      <td>0.949983</td>\n",
       "      <td>0.949897</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.955545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.195200</td>\n",
       "      <td>0.297350</td>\n",
       "      <td>0.952425</td>\n",
       "      <td>0.952428</td>\n",
       "      <td>0.952601</td>\n",
       "      <td>0.952425</td>\n",
       "      <td>0.943993</td>\n",
       "      <td>0.961161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.204300</td>\n",
       "      <td>0.285874</td>\n",
       "      <td>0.952655</td>\n",
       "      <td>0.952657</td>\n",
       "      <td>0.952685</td>\n",
       "      <td>0.952655</td>\n",
       "      <td>0.949864</td>\n",
       "      <td>0.955545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.155700</td>\n",
       "      <td>0.278088</td>\n",
       "      <td>0.954723</td>\n",
       "      <td>0.954726</td>\n",
       "      <td>0.954773</td>\n",
       "      <td>0.954723</td>\n",
       "      <td>0.950768</td>\n",
       "      <td>0.958821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>0.272843</td>\n",
       "      <td>0.956102</td>\n",
       "      <td>0.956105</td>\n",
       "      <td>0.956193</td>\n",
       "      <td>0.956102</td>\n",
       "      <td>0.950316</td>\n",
       "      <td>0.962096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.113800</td>\n",
       "      <td>0.267911</td>\n",
       "      <td>0.955642</td>\n",
       "      <td>0.955644</td>\n",
       "      <td>0.955650</td>\n",
       "      <td>0.955642</td>\n",
       "      <td>0.954833</td>\n",
       "      <td>0.956481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.074500</td>\n",
       "      <td>0.266215</td>\n",
       "      <td>0.956332</td>\n",
       "      <td>0.956335</td>\n",
       "      <td>0.956386</td>\n",
       "      <td>0.956332</td>\n",
       "      <td>0.952123</td>\n",
       "      <td>0.960693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1485, training_loss=1.4692542721526791, metrics={'train_runtime': 2731.3799, 'train_samples_per_second': 34.835, 'train_steps_per_second': 0.544, 'total_flos': 1134968648785920.0, 'train_loss': 1.4692542721526791, 'epoch': 2.994452849218356})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "312c99b9c0d8561d",
   "metadata": {},
   "source": [
    "Save the results, the model and the state of the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "2488383c06dfedb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T20:47:39.484786Z",
     "start_time": "2025-01-13T20:46:56.366851Z"
    }
   },
   "source": [
    "trainer.evaluate(dataset_encoded['valid'])\n",
    "trainer.save_model(your_path)\n",
    "trainer.save_state()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='273' max='273' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [273/273 00:42]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
